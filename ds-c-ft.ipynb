{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q transformers datasets peft bitsandbytes accelerate","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T19:19:22.884852Z","iopub.execute_input":"2025-08-06T19:19:22.885148Z","iopub.status.idle":"2025-08-06T19:20:38.630384Z","shell.execute_reply.started":"2025-08-06T19:19:22.885126Z","shell.execute_reply":"2025-08-06T19:20:38.629257Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport numpy as np\nfrom datasets import load_dataset\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments, EvalPrediction, BitsAndBytesConfig\nfrom peft import LoraConfig, get_peft_model, TaskType, AutoPeftModelForCausalLM\nimport torch\nimport gc\nimport re\n\n# Ensure CUDA is available\nif not torch.cuda.is_available():\n    raise SystemError(\"CUDA is not available. This script requires a CUDA-enabled GPU.\")\n\ntorch.backends.cuda.matmul.allow_tf32 = True\ntorch.backends.cudnn.benchmark = True\n\n# --- Hardcode your Hugging Face and Weights & Biases tokens ---\nos.environ[\"HF_TOKEN\"] = \"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\"\nos.environ[\"WANDB_API_KEY\"] = \"63ee3d8971de42124013ebdb34cda9d303028b54\"\n\n# Initialize Weights & Biases (optional, but good for tracking)\nimport wandb\nwandb.login(key=os.environ[\"WANDB_API_KEY\"])\n\n# --- 1. Data Loading ---\nprint(\"Loading mathematics dataset from davidheineman/deepmind-math-large...\")\ndataset = load_dataset(\"davidheineman/deepmind-math-large\", split='train')\ntrain_test_split = dataset.train_test_split(test_size=0.005, seed=123)\ntrain_dataset = train_test_split[\"train\"]\ntest_dataset = train_test_split[\"test\"]\nprint(f\"Number of training examples: {len(train_dataset)}\")\nprint(f\"Number of test examples: {len(test_dataset)}\")\n\ntrain = train_dataset.map(lambda ex: {\"prompt\": f\"Question: {ex['question']}\\\\nAnswer:\", \"completion\": \" \" + ex['answer']})\ntest = test_dataset.map(lambda ex: {\"prompt\": f\"Question: {ex['question']}\\\\nAnswer:\", \"completion\": \" \" + ex['answer']})\n\nprint(\"Example from training data:\")\nprint(train[0])\n\n# --- 2. Tokenization and Training ---\nmodel_name = \"deepseek-ai/deepseek-coder-1.3b-instruct\"\ntokenizer = AutoTokenizer.from_pretrained(model_name, token=os.environ[\"HF_TOKEN\"])\ntokenizer.pad_token = tokenizer.eos_token\n\ndef tokenize_fn(ex):\n    tok = tokenizer(\n        ex[\"prompt\"] + ex[\"completion\"],\n        truncation=True,\n        max_length=128,\n        padding=\"max_length\"\n    )\n    tok[\"labels\"] = tok[\"input_ids\"].copy()\n    return tok\n\ntrain_tok = train.map(tokenize_fn, batched=True, remove_columns=train.column_names)\ntest_tok = test.map(tokenize_fn, batched=True, remove_columns=test.column_names)\n\ndef compute_metrics(eval_preds: EvalPrediction):\n    predictions, labels = eval_preds\n    predictions = np.argmax(predictions, axis=-1)\n    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n    decoded_preds = [pred.strip() for pred in decoded_preds]\n    decoded_labels = [label.strip() for label in decoded_labels]\n    exact_match = sum(1 for pred, label in zip(decoded_preds, decoded_labels) if pred == label) / len(decoded_preds)\n    return {\"exact_match\": exact_match}\n\n# Define the LoRA configuration once\nlora_config = LoraConfig(\n    r=8,\n    lora_alpha=16,\n    target_modules=[\"q_proj\", \"v_proj\"],\n    lora_dropout=0.1,\n    bias=\"none\",\n    task_type=TaskType.CAUSAL_LM\n)\n\n# Define BitsAndBytes config once\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.float16,\n    bnb_4bit_use_double_quant=False,\n)\n\n# --- 3. Refactored Training Loop ---\noutput_dir = \"./deepseek_mathpatch\"\nos.makedirs(output_dir, exist_ok=True)\n\n# Find the latest checkpoint if it exists\ndef get_latest_checkpoint(output_dir):\n    checkpoints = [d for d in os.listdir(output_dir) if d.startswith(\"checkpoint-\")]\n    if not checkpoints:\n        return None\n    checkpoint_numbers = [int(re.search(r'\\d+', c).group()) for c in checkpoints]\n    latest_checkpoint_number = max(checkpoint_numbers)\n    return os.path.join(output_dir, f\"checkpoint-{latest_checkpoint_number}\")\n\nlatest_checkpoint = get_latest_checkpoint(output_dir)\n\nif latest_checkpoint:\n    print(f\"Resuming training from latest checkpoint: {latest_checkpoint}\")\n    model = AutoPeftModelForCausalLM.from_pretrained(\n        latest_checkpoint,\n        torch_dtype=torch.float16,\n        use_cache=False,\n        token=os.environ[\"HF_TOKEN\"]\n    )\nelse:\n    print(\"No checkpoint found. Starting training from scratch.\")\n    model = AutoModelForCausalLM.from_pretrained(\n        model_name,\n        quantization_config=bnb_config,\n        torch_dtype=torch.float16,\n        token=os.environ[\"HF_TOKEN\"],\n        use_cache=False\n    )\n    model = get_peft_model(model, lora_config)\n\nmodel.enable_input_require_grads()\nmodel.print_trainable_parameters()\n\ntraining_args = TrainingArguments(\n    output_dir=output_dir,\n    per_device_train_batch_size=1,\n    per_device_eval_batch_size=1,\n    gradient_accumulation_steps=8,\n    learning_rate=1e-4,\n    num_train_epochs=1,\n    logging_steps=50,\n    eval_strategy=\"steps\",\n    save_strategy=\"steps\",\n    eval_steps=245,\n    save_steps=245,  # Save every 250 steps\n    save_total_limit=3, # Keep a few checkpoints\n    fp16=True,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"exact_match\",\n    greater_is_better=True,\n    report_to=\"wandb\",\n    run_name=\"deepseek-math-finetune\",\n    gradient_checkpointing=True,\n    # Set max_steps to the total number of steps you want to train for\n    # The Trainer will handle the rest\n    #max_steps=9478, \n)\n\ntrainer = Trainer(\n    model=model,\n    tokenizer=tokenizer,\n    args=training_args,\n    train_dataset=train_tok,\n    eval_dataset=test_tok,\n    compute_metrics=compute_metrics,\n)\n\n# Start training. The Trainer will handle resuming from the last checkpoint automatically.\ntrainer.train(resume_from_checkpoint=latest_checkpoint)\n\nprint(\"Full training process completed.\")\n\n# Final Evaluation and Inference with the best model\nprint(\"\\n--- Final Model Evaluation and Prediction ---\")\nlatest_checkpoint = get_latest_checkpoint(output_dir)\nif latest_checkpoint:\n    final_model = AutoPeftModelForCausalLM.from_pretrained(\n        latest_checkpoint,\n        torch_dtype=torch.float16,\n        token=os.environ[\"HF_TOKEN\"],\n        use_cache=True\n    )\n    final_trainer = Trainer(\n        model=final_model,\n        tokenizer=tokenizer,\n        args=training_args,\n    )\n\n    eval_results = final_trainer.evaluate(eval_dataset=test_tok)\n    print(f\"Final Evaluation Results: {eval_results}\")\n\n    print(\"\\n--- Testing Final Inference ---\")\n    test_question = \"What is (15 + 27)?\"\n    inputs = tokenizer(f\"Question: {test_question}\\nAnswer:\", return_tensors=\"pt\").to(final_model.device)\n    with torch.no_grad():\n        outputs = final_model.generate(\n            **inputs,\n            max_new_tokens=10,\n            do_sample=False,\n            pad_token_id=tokenizer.eos_token_id\n        )\n    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    print(f\"Input: {test_question}\")\n    print(f\"Output: {response}\")\n\n# Finish the Weights & Biases run\nwandb.finish()","metadata":{"id":"2fm7Rkq53Z5B","trusted":true,"execution":{"iopub.status.busy":"2025-08-06T19:20:43.897204Z","iopub.execute_input":"2025-08-06T19:20:43.897550Z","iopub.status.idle":"2025-08-06T19:24:27.864050Z","shell.execute_reply.started":"2025-08-06T19:20:43.897523Z","shell.execute_reply":"2025-08-06T19:24:27.862934Z"}},"outputs":[],"execution_count":null}]}